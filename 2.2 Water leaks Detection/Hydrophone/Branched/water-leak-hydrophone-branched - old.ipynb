{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-27T19:02:36.954386Z","iopub.execute_input":"2023-08-27T19:02:36.954768Z","iopub.status.idle":"2023-08-27T19:02:37.054450Z","shell.execute_reply.started":"2023-08-27T19:02:36.954735Z","shell.execute_reply":"2023-08-27T19:02:37.053527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sklearn\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, SimpleRNN, GRU\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas_profiling import ProfileReport\n\n# import warnings\n# warnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:37.056474Z","iopub.execute_input":"2023-08-27T19:02:37.057426Z","iopub.status.idle":"2023-08-27T19:02:37.072494Z","shell.execute_reply.started":"2023-08-27T19:02:37.057399Z","shell.execute_reply":"2023-08-27T19:02:37.071435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/leak-detection/Dataset for Leak Detection and Localization in Water Distribution Systems/Dataset for Leak Detection and Localization in Water Distribution Systems/Hydrophone/Branched/Circumferential Crack/\"\nfile_extension = \".raw\"\nsampling_rate = 8000\n\nall_data = []\n\n# Loop through all .raw files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(file_extension):\n        file_path = os.path.join(data_dir, filename)\n        \n        with open(file_path, \"rb\") as file:\n            file_content = file.read()\n            \n        # Convert binary data to NumPy array that will be a dataframe\n        hydrophone_data =  pd.DataFrame(np.frombuffer(file_content, dtype=np.int16))\n        \n        all_data.append(hydrophone_data)\n        \n# Concatenate all DataFrames into a single DataFrame\ndf = pd.concat(all_data, axis=1, ignore_index=True)\n\ndf[\"LeakType\"] = \"Circumferential Crack\"","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:37.076038Z","iopub.execute_input":"2023-08-27T19:02:37.076756Z","iopub.status.idle":"2023-08-27T19:02:37.303865Z","shell.execute_reply.started":"2023-08-27T19:02:37.076724Z","shell.execute_reply":"2023-08-27T19:02:37.302865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/leak-detection/Dataset for Leak Detection and Localization in Water Distribution Systems/Dataset for Leak Detection and Localization in Water Distribution Systems/Hydrophone/Branched/Gasket Leak/\"\nfile_extension = \".raw\"\nsampling_rate = 8000\n\nall_data = []\n\n# Loop through all .raw files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(file_extension):\n        file_path = os.path.join(data_dir, filename)\n        \n        with open(file_path, \"rb\") as file:\n            file_content = file.read()\n            \n        # Convert binary data to NumPy array that will be a dataframe\n        hydrophone_data =  pd.DataFrame(np.frombuffer(file_content, dtype=np.int16))\n        \n        all_data.append(hydrophone_data)\n        \n# Concatenate all DataFrames into a single DataFrame\ndf1 = pd.concat(all_data, axis=1, ignore_index=True)\n\ndf1[\"LeakType\"] = \"Gasket Leak\"","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:37.306703Z","iopub.execute_input":"2023-08-27T19:02:37.307131Z","iopub.status.idle":"2023-08-27T19:02:37.537194Z","shell.execute_reply.started":"2023-08-27T19:02:37.307049Z","shell.execute_reply":"2023-08-27T19:02:37.536145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/leak-detection/Dataset for Leak Detection and Localization in Water Distribution Systems/Dataset for Leak Detection and Localization in Water Distribution Systems/Hydrophone/Branched/Longitudinal Crack/\"\nfile_extension = \".raw\"\nsampling_rate = 8000\n\nall_data = []\n\n# Loop through all .raw files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(file_extension):\n        file_path = os.path.join(data_dir, filename)\n        \n        with open(file_path, \"rb\") as file:\n            file_content = file.read()\n            \n        # Convert binary data to NumPy array that will be a dataframe\n        hydrophone_data =  pd.DataFrame(np.frombuffer(file_content, dtype=np.int16))\n        \n        all_data.append(hydrophone_data)\n        \n# Concatenate all DataFrames into a single DataFrame\ndf2 = pd.concat(all_data, axis=1, ignore_index=True)\n\ndf2[\"LeakType\"] = \"Longitudinal Crack\"","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:37.538963Z","iopub.execute_input":"2023-08-27T19:02:37.539322Z","iopub.status.idle":"2023-08-27T19:02:37.753794Z","shell.execute_reply.started":"2023-08-27T19:02:37.539288Z","shell.execute_reply":"2023-08-27T19:02:37.752822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/leak-detection/Dataset for Leak Detection and Localization in Water Distribution Systems/Dataset for Leak Detection and Localization in Water Distribution Systems/Hydrophone/Branched/NonLeak/\"\nfile_extension = \".raw\"\nsampling_rate = 8000\n\nall_data = []\n\n# Loop through all .raw files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(file_extension):\n        file_path = os.path.join(data_dir, filename)\n        \n        with open(file_path, \"rb\") as file:\n            file_content = file.read()\n            \n        # Convert binary data to NumPy array that will be a dataframe\n        hydrophone_data =  pd.DataFrame(np.frombuffer(file_content, dtype=np.int16))\n        \n        all_data.append(hydrophone_data)\n        \n# Concatenate all DataFrames into a single DataFrame\ndf3 = pd.concat(all_data, axis=1, ignore_index=True)\n\ndf3[\"LeakType\"] = \"NonLeak\"","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:37.755498Z","iopub.execute_input":"2023-08-27T19:02:37.755895Z","iopub.status.idle":"2023-08-27T19:02:38.075945Z","shell.execute_reply.started":"2023-08-27T19:02:37.755860Z","shell.execute_reply":"2023-08-27T19:02:38.074881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"/kaggle/input/leak-detection/Dataset for Leak Detection and Localization in Water Distribution Systems/Dataset for Leak Detection and Localization in Water Distribution Systems/Hydrophone/Branched/Orifice Leak/\"\nfile_extension = \".raw\"\nsampling_rate = 8000\n\nall_data = []\n\n# Loop through all .raw files in the directory\nfor filename in os.listdir(data_dir):\n    if filename.endswith(file_extension):\n        file_path = os.path.join(data_dir, filename)\n        \n        with open(file_path, \"rb\") as file:\n            file_content = file.read()\n            \n        # Convert binary data to NumPy array that will be a dataframe\n        hydrophone_data =  pd.DataFrame(np.frombuffer(file_content, dtype=np.int16))\n        \n        all_data.append(hydrophone_data)\n        \n# Concatenate all DataFrames into a single DataFrame\ndf4 = pd.concat(all_data, axis=1, ignore_index=True)\n\ndf4[\"LeakType\"] = \"Orifice Leak\"","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:38.077286Z","iopub.execute_input":"2023-08-27T19:02:38.077661Z","iopub.status.idle":"2023-08-27T19:02:38.259129Z","shell.execute_reply.started":"2023-08-27T19:02:38.077623Z","shell.execute_reply":"2023-08-27T19:02:38.258011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([df, df1, df2, df3, df4], axis=0)\ndata = data.sample(frac = 1)\ndata","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:38.260884Z","iopub.execute_input":"2023-08-27T19:02:38.261236Z","iopub.status.idle":"2023-08-27T19:02:39.574575Z","shell.execute_reply.started":"2023-08-27T19:02:38.261197Z","shell.execute_reply":"2023-08-27T19:02:39.573492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:39.576306Z","iopub.execute_input":"2023-08-27T19:02:39.576680Z","iopub.status.idle":"2023-08-27T19:02:39.589300Z","shell.execute_reply.started":"2023-08-27T19:02:39.576647Z","shell.execute_reply":"2023-08-27T19:02:39.588167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:39.594066Z","iopub.execute_input":"2023-08-27T19:02:39.594593Z","iopub.status.idle":"2023-08-27T19:02:42.261197Z","shell.execute_reply.started":"2023-08-27T19:02:39.594563Z","shell.execute_reply":"2023-08-27T19:02:42.259889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:42.263083Z","iopub.execute_input":"2023-08-27T19:02:42.263484Z","iopub.status.idle":"2023-08-27T19:02:43.407002Z","shell.execute_reply.started":"2023-08-27T19:02:42.263448Z","shell.execute_reply":"2023-08-27T19:02:43.406110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:43.408309Z","iopub.execute_input":"2023-08-27T19:02:43.408673Z","iopub.status.idle":"2023-08-27T19:02:44.546774Z","shell.execute_reply.started":"2023-08-27T19:02:43.408639Z","shell.execute_reply":"2023-08-27T19:02:44.545750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()\ndata.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:44.548020Z","iopub.execute_input":"2023-08-27T19:02:44.548878Z","iopub.status.idle":"2023-08-27T19:02:46.655478Z","shell.execute_reply.started":"2023-08-27T19:02:44.548843Z","shell.execute_reply":"2023-08-27T19:02:46.654467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.shape)\ndata = data.drop_duplicates()\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:46.656771Z","iopub.execute_input":"2023-08-27T19:02:46.657215Z","iopub.status.idle":"2023-08-27T19:02:48.522004Z","shell.execute_reply.started":"2023-08-27T19:02:46.657187Z","shell.execute_reply":"2023-08-27T19:02:48.520986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:48.523532Z","iopub.execute_input":"2023-08-27T19:02:48.523917Z","iopub.status.idle":"2023-08-27T19:02:49.188029Z","shell.execute_reply.started":"2023-08-27T19:02:48.523883Z","shell.execute_reply":"2023-08-27T19:02:49.186877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndata['category_label_encoded'] = label_encoder.fit_transform(data['LeakType'])","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:49.189725Z","iopub.execute_input":"2023-08-27T19:02:49.190113Z","iopub.status.idle":"2023-08-27T19:02:49.580783Z","shell.execute_reply.started":"2023-08-27T19:02:49.190079Z","shell.execute_reply":"2023-08-27T19:02:49.579754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(['LeakType', 'category_label_encoded'], axis=1)  # Features\ny = pd.DataFrame(data['category_label_encoded'])  # Target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:49.582082Z","iopub.execute_input":"2023-08-27T19:02:49.582430Z","iopub.status.idle":"2023-08-27T19:02:49.886317Z","shell.execute_reply.started":"2023-08-27T19:02:49.582397Z","shell.execute_reply":"2023-08-27T19:02:49.885289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:49.887971Z","iopub.execute_input":"2023-08-27T19:02:49.888350Z","iopub.status.idle":"2023-08-27T19:02:50.083046Z","shell.execute_reply.started":"2023-08-27T19:02:49.888314Z","shell.execute_reply":"2023-08-27T19:02:50.081821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build DNN model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))  # Output layer, 5 for the number of unique categories\n\n# Compile model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:50.084720Z","iopub.execute_input":"2023-08-27T19:02:50.085760Z","iopub.status.idle":"2023-08-27T19:02:50.137752Z","shell.execute_reply.started":"2023-08-27T19:02:50.085718Z","shell.execute_reply":"2023-08-27T19:02:50.136871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhistory = model.fit(X_train_scaled, y_train, epochs=5, batch_size=16, validation_data=(X_test_scaled, y_test))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:02:50.139202Z","iopub.execute_input":"2023-08-27T19:02:50.139537Z","iopub.status.idle":"2023-08-27T19:23:45.027654Z","shell.execute_reply.started":"2023-08-27T19:02:50.139506Z","shell.execute_reply":"2023-08-27T19:23:45.026854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\ntest_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy:.4f}')\n\n# Make predictions\ny_pred = model.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:23:45.029145Z","iopub.execute_input":"2023-08-27T19:23:45.029491Z","iopub.status.idle":"2023-08-27T19:24:26.141663Z","shell.execute_reply.started":"2023-08-27T19:23:45.029456Z","shell.execute_reply":"2023-08-27T19:24:26.140658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\nconf_matrix = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\nprint(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T19:33:45.146581Z","iopub.execute_input":"2023-08-27T19:33:45.149782Z","iopub.status.idle":"2023-08-27T19:33:45.209912Z","shell.execute_reply.started":"2023-08-27T19:33:45.149746Z","shell.execute_reply":"2023-08-27T19:33:45.208825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}